# -*- coding: utf-8 -*-
"""applicantData.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wwLERfLyMjECGTmHsaS9iXyn3xILATce

# ANALYSIS BY STANLEY UZUM
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pandas import Timestamp
import warnings
import plotly.graph_objects as go
import plotly.offline as pyo
import plotly.io as pio
import sklearn
import plotly.express as ex
warnings.filterwarnings('ignore')

data = pd.read_excel("C:/Users/tbigg/Downloads/Applicants_data.xlsx")

data

data.nunique()

#Checking percentage of null values

(data.isnull().sum()/(len(data)))*100

data.info()

"""## Handling Missing Value"""

# Fill missing numerical values with mean

numerical_columns = data.select_dtypes(include=['number']).columns
data[numerical_columns] = data[numerical_columns].fillna(data[numerical_columns].mean())

data.isnull().sum()

# Fill missing categorical values with mode

categorical_columns = data.select_dtypes(include=['object']).columns
for col in categorical_columns:
    data[col].fillna(data[col].mode()[0], inplace=True)

data.isnull().sum()

data

# Select categorical columns
categorical_columns = data.select_dtypes(include=['object']).columns

# Print unique values for each categorical column
for col in categorical_columns:
    unique_values = data[col].unique()
    print(f"Unique values for {col}: {unique_values}")

"""## Feature Enginnering

to Create new features that may be predictive of churn. For example, i could calculate metrics such as:
 - average distance covered per scan,
 - average steps taken per scan,
 - frequency of scans, etc.
Time-based features could also be useful, such as:
- the time of day or day of the week when the scan was performed.

After performing feature engineering, the target variable for our churn prediction model would typically be a binary variable indicating whether a user has churned or not. This variable could be derived from the available data based on a specific definition of churn.

Here are a few common ways to define churn and derive the target variable:

Time-based Churn: Define churn based on a period of inactivity or non-usage. For example, if a user has not performed any scans or activities within the app for a certain period (e.g., 30 days), they could be considered as churned.

Behavior-based Churn: Define churn based on a significant decrease in activity compared to a user's historical behavior. For example, if a user's scan frequency or distance covered drops below a certain threshold compared to their past behavior, they could be considered as churned.

Event-based Churn: Define churn based on specific events or actions indicating disengagement. For example, if a user unsubscribes from notifications, deletes their account, or performs a certain action indicating dissatisfaction or intent to leave the platform, they could be considered as churned.

Once you have defined churn based on one of these criteria, you can derive the target variable by labeling users as "churned" (1) if they meet the churn criteria and "not churned" (0) otherwise. This target variable will be used to train and evaluate your churn prediction model.
"""

# Frequency of Scans
data['total_scans'] = data.groupby('team_id')['team_id'].transform('count')
data['Created_at'] = pd.to_datetime(data['created_at'])  # Convert 'Created_at' to datetime if it's not already
data['min_created_at'] = data.groupby('team_id')['created_at'].transform('min')
data['days_since_first_scan'] = (data['created_at'] - data['min_created_at']).dt.days
data['avg_scans_per_day'] = data['total_scans'] / data['days_since_first_scan']


# Average Distance Covered per Scan
data['avg_distance_per_scan'] = data['distance'] / data['total_scans']

# Average Steps Taken per Scan
data['avg_steps_per_scan'] = data['steps'] / data['total_scans']

# Time-based Features
data['hour_of_day'] = pd.to_datetime(data['created_at']).dt.hour
data['day_of_week'] = pd.to_datetime(data['created_at']).dt.dayofweek
data['is_peak_hour'] = ((data['hour_of_day'] >= 7) & (data['hour_of_day'] <= 9)) | ((data['hour_of_day'] >= 17) & (data['hour_of_day'] <= 19))
data['is_weekend'] = data['day_of_week'].isin([5, 6])
data['time_since_first_scan'] = (pd.to_datetime(data['created_at']) - data.groupby('team_id')['created_at'].transform('min')).dt.days


# Interaction Features
data['distance_steps_interaction'] = data['distance'] * data['steps']

data

"""##


1. **total_scans**: This variable represents the total number of scans performed by each team. It's calculated by grouping the DataFrame by 'team_id' and then counting the occurrences of each 'team_id'.

2. **Created_at**: This variable represents the timestamp when each scan was performed. It's converted to datetime format using `pd.to_datetime()`.

3. **min_created_at**: This variable represents the minimum 'Created_at' timestamp for each team. It's calculated by grouping the DataFrame by 'team_id' and finding the minimum timestamp for each group.

4. **days_since_first_scan**: This variable represents the number of days elapsed since the first scan for each team. It's calculated by subtracting the 'min_created_at' timestamp from the 'Created_at' timestamp and extracting the number of days using the `.dt.days` accessor.

5. **avg_scans_per_day**: This variable represents the average number of scans per day for each team. It's calculated by dividing the 'total_scans' by the 'days_since_first_scan'.

6. **avg_distance_per_scan**: This variable represents the average distance covered per scan for each team. It's calculated by dividing the 'distance' by the 'total_scans'.

7. **avg_steps_per_scan**: This variable represents the average number of steps taken per scan for each team. It's calculated by dividing the 'steps' by the 'total_scans'.

8. **hour_of_day**: This variable represents the hour of the day when each scan was performed. It's extracted from the 'Created_at' timestamp using `.dt.hour`.

9. **day_of_week**: This variable represents the day of the week (0 = Monday, 1 = Tuesday, ..., 6 = Sunday) when each scan was performed. It's extracted from the 'Created_at' timestamp using `.dt.dayofweek`.

10. **is_peak_hour**: This variable is a binary indicator (True/False) representing whether each scan was performed during peak hours of Dusk and Dawn (7-9 AM or 5-7 PM). It's based on the 'hour_of_day' variable.

11. **is_weekend**: This variable is a binary indicator (True/False) representing whether each scan was performed on a weekend (Saturday or Sunday). It's based on the 'day_of_week' variable.

12. **time_since_first_scan**: This variable represents the number of days elapsed since the first scan for each team. It's calculated by subtracting the 'min_created_at' timestamp from the 'Created_at' timestamp and extracting the number of days using the `.dt.days` accessor.

13. **distance_steps_interaction**: This variable represents the interaction between 'distance' and 'steps' for each scan. It's calculated by multiplying the 'distance' by the 'steps'.

These variables capture various aspects of user behavior, engagement, and context that may be predictive of churn. They can be used as features in a churn prediction model to identify patterns and trends associated with user churn.
"""

# Drop unnecessary columns
data.drop(['Qid', 'Created_at'], axis=1, inplace=True)

data.nunique()

data.total_scans.unique()

# Set display options to show all columns
pd.set_option('display.max_columns', None)

# Print the DataFrame
data.head(30)

# Define churn threshold (e.g., if a user has fewer than 10 scans in the last 30 days)
scans_threshold = 10
days_threshold = 30

# Create a new column to determine churn
data['churn'] = ((data['total_scans'] < scans_threshold) & (data['days_since_first_scan'] >= days_threshold)).astype(int)

data

"""##

In this code:

I define a threshold for churn, such as having fewer than 10 scans in the last 30 days.
I create a new column 'churn' where we classify users as churned (1) if they meet the churn criteria or not churned (0) otherwise.
thresholds can be adjusted based on the Order/Request from Stalkholders or Supervisor.

## Feature Selection
"""

plt.figure(figsize=(20, 10))
sns.heatmap(data.drop(['is_weekend', 'is_peak_hour', 'Month Name', 'scanType',
       'scan_mode', 'player_id'],axis=1).corr(), annot = True, vmin = -1, vmax = 1)
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Select all numerical columns as features
numerical_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
target = 'churn'

# Check if the target variable is in the list of numerical features before removing it
if target in numerical_features:
    numerical_features.remove(target)
else:
    print("Target variable not found in numerical features.")

# Preprocess numerical features to handle infinite or very large values
for feature in numerical_features:
    data[feature] = data[feature].replace([np.inf, -np.inf], np.nan)  # Replace infinite values with NaN
    data[feature] = data[feature].fillna(data[feature].mean())  # Replace NaN with mean of the feature

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(data[numerical_features], data[target], test_size=0.2, random_state=42)

# Train a random forest classifier
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train, y_train)

# Get feature importance
feature_importance = pd.Series(rf_classifier.feature_importances_, index=numerical_features)

# Sort feature importance in descending order
feature_importance_sorted = feature_importance.sort_values(ascending=False)

# Select top features
top_features = feature_importance_sorted.index.tolist()

# Print top features
print("Top Features:")
print(top_features)

"""## Predictive Model"""

x = data[['avg_scans_per_day', 'days_since_first_scan', 'time_since_first_scan', 'total_scans',
          'avg_steps_per_scan', 'avg_distance_per_scan', 'team_id', 'lat', 'distance_steps_interaction',
          'qid', 'lng', 'steps', 'Month', 'distance', 'Year']]
y = data['churn']

"""##

 Since churn prediction is often imbalanced (fewer churners compared to non-churners), i will consider using techniques like stratified sampling.
"""

from sklearn.model_selection import train_test_split

# Perform stratified sampling
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""## Feature Scaling
Effect of Feature Scale:

 - Standardization can help algorithms converge faster and can make the optimization process more stable.
 - It can also prevent features with larger scales from dominating the optimization process, especially in algorithms that use distance metrics or gradient-based optimization.
"""

from sklearn.preprocessing import StandardScaler
stndrd = StandardScaler()
X_train = stndrd.fit_transform(X_train)
X_test = stndrd.transform(X_test)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.metrics import classification_report, confusion_matrix

# Define the model
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Predict on the validation set
y_pred_nn = (model.predict(X_test) > 0.5).astype("int32")

# Evaluate the model
print("Neural Network Evaluation:")
print(classification_report(y_test, y_pred_nn))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_nn))

"""## Gradient Boosting (using XGBoost):"""

import xgboost as xgb
from sklearn.metrics import classification_report, confusion_matrix

# Define the model
xgb_model = xgb.XGBClassifier()

# Train the model
xgb_model.fit(X_train, y_train)

# Predict on the validation set
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model
print("Gradient Boosting Evaluation:")
print(classification_report(y_test, y_pred_xgb))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_xgb))

"""## Logistic Regression:"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Define the model
logreg_model = LogisticRegression()

# Train the model
logreg_model.fit(X_train, y_train)

# Predict on the validation set
y_pred_lr = logreg_model.predict(X_test)

# Evaluate the model
print("Logistic Regression Evaluation:")
print(classification_report(y_test, y_pred_lr))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_lr))

"""## Support Vector Machines (SVM)"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

# Define the model
svm_model = SVC(kernel='linear')

# Train the model
svm_model.fit(X_train, y_train)

# Predict on the validation set
y_pred_svm = svm_model.predict(X_test)

# Evaluate the model
print("Support Vector Machines (SVM) Evaluation:")
print(classification_report(y_test, y_pred_svm))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_svm))

"""## Visualizing Confusion Matrix"""

from sklearn.metrics import confusion_matrix

# Define a function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(title)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

# Call the function for each model
plot_confusion_matrix(y_test, y_pred_nn, title='Neural Network Confusion Matrix')
plot_confusion_matrix(y_test, y_pred_xgb, title='Gradient Boosting Confusion Matrix')
plot_confusion_matrix(y_test, y_pred_lr, title='Logistic Regression Confusion Matrix')
plot_confusion_matrix(y_test, y_pred_svm, title='SVM Confusion Matrix')

